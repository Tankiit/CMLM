{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from src.slurm import init_signal_handler, init_distributed_mode\n",
    "from src.data.loader import check_data_params, load_data\n",
    "from src.utils import bool_flag, initialize_exp, set_sampling_probs, shuf_order\n",
    "from src.model import check_model_params, build_model\n",
    "from src.trainer import SingleTrainer\n",
    "from src.evaluation.evaluator import SingleEvaluator\n",
    "\n",
    "import apex\n",
    "from src.fp16 import network_to_half\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument inputs (works in JUPYTER environment ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## jupyter\n",
    "## main parameters\n",
    "def modified_params(params):\n",
    "    params.exp_name = \"cmlm\"                       # experiment name\n",
    "    params.dump_path = \"./dumped/\"                 # where to store the experiment\n",
    "\n",
    "    ## data location / training objective\n",
    "    params.data_path = \"./data/processed/coco/\"    # data location\n",
    "    params.lngs = 'wiki'                           # lang pretraining source\n",
    "    params.imgs = 'vg'                             # image pretraining source\n",
    "    params.cmodal = 'cap-img'                      # multi-modal\n",
    "    params.mlm_steps = 'wiki'                      # MLM objective for language\n",
    "    params.ipm_steps = 'vg'                        # MLM objective for image\n",
    "    params.cmlm_steps = 'cap-img'                  # CMLM objective\n",
    "    \n",
    "    ## image\n",
    "    params.n_bbox = 36                             # Number of bounding boxes\n",
    "    params.spatial_feat = 6                        # Number of spatial features\n",
    "    params.features = 2048                         # Dimension of image feature vectors\n",
    "\n",
    "    ## transformer parameters\n",
    "    params.emb_dim = 1024                          # embeddings / model dimension\n",
    "    params.n_layers = 6                            # number of layers\n",
    "    params.n_heads = 8                             # number of heads\n",
    "    params.dropout = 0.1                           # dropout\n",
    "    params.attention_dropout = 0.1                 # attention dropout\n",
    "    params.gelu_activation = True                  # GELU instead of ReLU\n",
    "\n",
    "    ## optimization\n",
    "    params.batch_size = 32                           # sequences per batch\n",
    "    params.bptt_word = 256                           # sequences length for language\n",
    "    params.bptt_img = 256                            # sequences length for image\n",
    "    params.optimizer = \"adam,lr=0.0001\"              # optimizer\n",
    "    params.epoch_size = 200000                       # number of sentences per epoch\n",
    "    params.validation_metrics = \"_valid_mlm_ppl,_ce\"     # validation metric (when to save the best model)\n",
    "    params.stopping_criterion = \"_valid_mlm_ppl;_ce,10\"  # end experiment if stopping criterion does not improve\n",
    "    \n",
    "#     params.stopping_criterion_image = \"_cross_entropy,10\"  # end experiment if stopping criterion does not improve\n",
    "#     params.stopping_criterion_cmodal = \"_combined,10\"      # end experiment if stopping criterion does not improve\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    \"\"\"\n",
    "    Generate a parameters parser.\n",
    "    \"\"\"\n",
    "    # parse parameters\n",
    "    parser = argparse.ArgumentParser(description=\"Cross-modal learning\")\n",
    "\n",
    "    # main parameters\n",
    "    parser.add_argument(\"--dump_path\", type=str, default=\"./dumped/\",\n",
    "                        help=\"Experiment dump path\")\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"\",\n",
    "                        help=\"Experiment name\")\n",
    "    parser.add_argument(\"--save_periodic\", type=int, default=0,\n",
    "                        help=\"Save the model periodically (0 to disable)\")\n",
    "    parser.add_argument(\"--exp_id\", type=str, default=\"\",\n",
    "                        help=\"Experiment ID\")\n",
    "    \n",
    "    # float16\n",
    "    parser.add_argument(\"--fp16\", type=bool_flag, default=False,\n",
    "                        help=\"Run model with float16\")\n",
    "\n",
    "    # only use an encoder (use a specific decoder for machine translation)\n",
    "    parser.add_argument(\"--encoder_only\", type=bool_flag, default=True,\n",
    "                        help=\"Only use an encoder\")\n",
    "    \n",
    "    # model parameters\n",
    "    parser.add_argument(\"--emb_dim\", type=int, default=512,\n",
    "                        help=\"Embedding layer size\")\n",
    "    parser.add_argument(\"--n_layers\", type=int, default=4,\n",
    "                        help=\"Number of Transformer layers\")\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=8,\n",
    "                        help=\"Number of Transformer heads\")\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "                        help=\"Dropout\")\n",
    "    parser.add_argument(\"--attention_dropout\", type=float, default=0,\n",
    "                        help=\"Dropout in the attention layer\")\n",
    "    parser.add_argument(\"--gelu_activation\", type=bool_flag, default=False,\n",
    "                        help=\"Use a GELU activation instead of ReLU\")\n",
    "    parser.add_argument(\"--share_inout_emb\", type=bool_flag, default=True,\n",
    "                        help=\"Share input and output embeddings\")\n",
    "    parser.add_argument(\"--sinusoidal_embeddings\", type=bool_flag, default=False,\n",
    "                        help=\"Use sinusoidal embeddings\")\n",
    "    \n",
    "    # adaptive softmax\n",
    "    parser.add_argument(\"--asm\", type=bool_flag, default=False,\n",
    "                        help=\"Use adaptive softmax\")\n",
    "    if parser.parse_known_args()[0].asm:\n",
    "        parser.add_argument(\"--asm_cutoffs\", type=str, default=\"8000,20000\",\n",
    "                            help=\"Adaptive softmax cutoffs\")\n",
    "        parser.add_argument(\"--asm_div_value\", type=float, default=4,\n",
    "                            help=\"Adaptive softmax cluster sizes ratio\")\n",
    "        \n",
    "    # causal language modeling task parameters\n",
    "    parser.add_argument(\"--context_size\", type=int, default=0,\n",
    "                        help=\"Context size (0 means that the first elements in sequences won't have any context)\")\n",
    "    \n",
    "    # masked language modeling task parameters\n",
    "    parser.add_argument(\"--word_pred\", type=float, default=0.15,\n",
    "                        help=\"Fraction of words for which we need to make a prediction\")\n",
    "    parser.add_argument(\"--word_sample_alpha\", type=float, default=0,\n",
    "                        help=\"Exponent for transforming word counts to probabilities (~word2vec sampling)\")\n",
    "    parser.add_argument(\"--word_mask_keep_rand\", type=str, default=\"0.8,0.1,0.1\",\n",
    "                        help=\"Fraction of words to mask out / keep / randomize, among the words to predict\")\n",
    "    \n",
    "    # masked image pretraining modeling task parameters\n",
    "    parser.add_argument(\"--img_pred\", type=float, default=0.15,\n",
    "                        help=\"Fraction of imgs for which we need to make a prediction\")\n",
    "    parser.add_argument(\"--img_sample_alpha\", type=float, default=0,\n",
    "                        help=\"Exponent for transforming img counts to probabilities. Should be zero\")\n",
    "    parser.add_argument(\"--img_mask_keep_rand\", type=str, default=\"0.8,0.1,0.1\",\n",
    "                        help=\"Fraction of img features to mask out / keep / randomize, among the features to predict\")\n",
    "    parser.add_argument(\"--n_bbox\", type=int, default=36,\n",
    "                        help=\"number of extracted bounding boxes\")\n",
    "    parser.add_argument(\"--features\", type=int, default=2048,\n",
    "                        help=\"dimension of feature vector for each region\") \n",
    "    \n",
    "    # input sentence noise\n",
    "    parser.add_argument(\"--word_shuffle\", type=float, default=0,\n",
    "                        help=\"Randomly shuffle input words (0 to disable)\")\n",
    "    parser.add_argument(\"--word_dropout\", type=float, default=0,\n",
    "                        help=\"Randomly dropout input words (0 to disable)\")\n",
    "    parser.add_argument(\"--word_blank\", type=float, default=0,\n",
    "                        help=\"Randomly blank input words (0 to disable)\")\n",
    "    \n",
    "    # data\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"\",\n",
    "                        help=\"Data path\")\n",
    "    parser.add_argument(\"--lngs\", type=str, default=\"\",\n",
    "                        help=\"Languages sources , wiki\")\n",
    "    parser.add_argument(\"--imgs\", type=str, default=\"\",\n",
    "                        help=\"Images sources , vg\")\n",
    "    parser.add_argument(\"--cmodal\", type=str, default=\"\",\n",
    "                        help=\"Crossmodal sources , cap-img\")\n",
    "    parser.add_argument(\"--max_vocab\", type=int, default=-1,\n",
    "                        help=\"Maximum vocabulary size (-1 to disable)\")\n",
    "    parser.add_argument(\"--min_count\", type=int, default=0,\n",
    "                        help=\"Minimum vocabulary count\")\n",
    "    parser.add_argument(\"--lg_sampling_factor\", type=float, default=-1,\n",
    "                        help=\"Language sampling factor\")\n",
    "    \n",
    "    # batch parameters for word\n",
    "    parser.add_argument(\"--bptt_word\", type=int, default=256,\n",
    "                        help=\"Sequence length\")\n",
    "    parser.add_argument(\"--max_len\", type=int, default=100,\n",
    "                        help=\"Maximum length of sentences (after BPE)\")\n",
    "    parser.add_argument(\"--group_by_size\", type=bool_flag, default=True,\n",
    "                        help=\"Sort sentences by size during the training\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                        help=\"Number of sentences per batch\")\n",
    "    parser.add_argument(\"--max_batch_size\", type=int, default=0,\n",
    "                        help=\"Maximum number of sentences per batch (used in combination with tokens_per_batch, 0 to disable)\")\n",
    "    parser.add_argument(\"--tokens_per_batch\", type=int, default=-1,\n",
    "                        help=\"Number of tokens per batch (we can control using tokens_per_batch or batch_size)\")\n",
    "    \n",
    "    # batch parameter for image\n",
    "    parser.add_argument(\"--bptt_img\", type=int, default=256,\n",
    "                        help=\"Sequence length\")\n",
    "    \n",
    "    # training parameters\n",
    "    parser.add_argument(\"--split_data\", type=bool_flag, default=False,\n",
    "                        help=\"Split data across workers of a same node\")\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"adam,lr=0.0001\",\n",
    "                        help=\"Optimizer (SGD / RMSprop / Adam, etc.)\")\n",
    "    parser.add_argument(\"--clip_grad_norm\", type=float, default=5,\n",
    "                        help=\"Clip gradients norm (0 to disable)\")\n",
    "    parser.add_argument(\"--epoch_size\", type=int, default=100000,\n",
    "                        help=\"Epoch size / evaluation frequency (-1 for parallel data size)\")\n",
    "    parser.add_argument(\"--max_epoch\", type=int, default=100000,\n",
    "                        help=\"Maximum epoch size\")\n",
    "    parser.add_argument(\"--stopping_criterion\", type=str, default=\"\",\n",
    "                        help=\"Stopping criterion, and number of non-increase before stopping the experiment\")\n",
    "    parser.add_argument(\"--validation_metrics\", type=str, default=\"\",\n",
    "                        help=\"Validation metrics\")\n",
    "    \n",
    "    # training coefficients\n",
    "    parser.add_argument(\"--lambda_mlm\", type=str, default=\"1\",\n",
    "                        help=\"Prediction coefficient (MLM)\")\n",
    "    parser.add_argument(\"--lambda_clm\", type=str, default=\"1\",\n",
    "                        help=\"Causal coefficient (LM)\")\n",
    "    parser.add_argument(\"--lambda_pc\", type=str, default=\"1\",\n",
    "                        help=\"PC coefficient\")\n",
    "    parser.add_argument(\"--lambda_ae\", type=str, default=\"1\",\n",
    "                        help=\"AE coefficient\")\n",
    "    parser.add_argument(\"--lambda_mt\", type=str, default=\"1\",\n",
    "                        help=\"MT coefficient\")\n",
    "    parser.add_argument(\"--lambda_bt\", type=str, default=\"1\",\n",
    "                        help=\"BT coefficient\")\n",
    "    parser.add_argument(\"--lambda_ipm\", type=str, default=\"1\",\n",
    "                        help=\"Caption coefficient\")\n",
    "    parser.add_argument(\"--lambda_cmlm\", type=str, default=\"1\",\n",
    "                        help=\"Caption-image coefficient\")\n",
    "    \n",
    "    # training steps\n",
    "    parser.add_argument(\"--mlm_steps\", type=str, default=\"\",\n",
    "                        help=\"Masked prediction steps (MLM / TLM)\")\n",
    "    parser.add_argument(\"--ipm_steps\", type=str, default=\"\",\n",
    "                        help=\"Masked image prediction steps (IPM)\")\n",
    "    parser.add_argument(\"--cmlm_steps\", type=str, default=\"\",\n",
    "                        help=\"Cross-modal prediction steps (CMLM)\")\n",
    "    parser.add_argument(\"--mt_steps\", type=str, default=\"\",\n",
    "                        help=\"Machine translation steps\")\n",
    "    parser.add_argument(\"--ae_steps\", type=str, default=\"\",\n",
    "                        help=\"Denoising auto-encoder steps\")\n",
    "    parser.add_argument(\"--bt_steps\", type=str, default=\"\",\n",
    "                        help=\"Back-translation steps\")\n",
    "    parser.add_argument(\"--pc_steps\", type=str, default=\"\",\n",
    "                        help=\"Parallel classification steps\")\n",
    "    \n",
    "    # reload pretrained embeddings / pretrained model / checkpoint\n",
    "    parser.add_argument(\"--reload_emb\", type=str, default=\"\",\n",
    "                        help=\"Reload pretrained word embeddings\")\n",
    "    parser.add_argument(\"--reload_model\", type=str, default=\"\",\n",
    "                        help=\"Reload a pretrained model\")\n",
    "    parser.add_argument(\"--reload_checkpoint\", type=str, default=\"\",\n",
    "                        help=\"Reload a checkpoint\")\n",
    "\n",
    "    # beam search (for MT only)\n",
    "    parser.add_argument(\"--beam_size\", type=int, default=1,\n",
    "                        help=\"Beam size, default = 1 (greedy decoding)\")\n",
    "    parser.add_argument(\"--length_penalty\", type=float, default=1,\n",
    "                        help=\"Length penalty, values < 1.0 favor shorter sentences, while values > 1.0 favor longer ones.\")\n",
    "    parser.add_argument(\"--early_stopping\", type=bool_flag, default=False,\n",
    "                        help=\"Early stopping, stop as soon as we have `beam_size` hypotheses, although longer ones may have better scores.\")\n",
    "    \n",
    "    # evaluation\n",
    "    parser.add_argument(\"--eval_bleu\", type=bool_flag, default=False,\n",
    "                        help=\"Evaluate BLEU score during MT training\")\n",
    "    parser.add_argument(\"--eval_only\", type=bool_flag, default=False,\n",
    "                        help=\"Only run evaluations\")\n",
    "    \n",
    "    # debug\n",
    "    parser.add_argument(\"--debug_train\", type=bool_flag, default=False,\n",
    "                        help=\"Use valid sets for train sets (faster loading)\")\n",
    "    parser.add_argument(\"--debug_slurm\", type=bool_flag, default=False,\n",
    "                        help=\"Debug multi-GPU / multi-node within a SLURM job\")\n",
    "    \n",
    "    # multi-gpu / multi-node\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                        help=\"Multi-GPU - Local rank\")\n",
    "    parser.add_argument(\"--master_port\", type=int, default=-1,\n",
    "                        help=\"Master port (for multi-node SLURM jobs)\")\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params):\n",
    "\n",
    "    # initialize the multi-GPU / multi-node training\n",
    "    init_distributed_mode(params)\n",
    "    \n",
    "    # initialize the experiment\n",
    "    logger = initialize_exp(params)\n",
    "\n",
    "    # initialize SLURM signal handler for time limit / pre-emption\n",
    "    init_signal_handler()\n",
    "\n",
    "    # load data\n",
    "    data = load_data(params)\n",
    "    \n",
    "    # build model\n",
    "    model = build_model(params, data['dico'])\n",
    "\n",
    "    # distributed\n",
    "    if params.multi_gpu:\n",
    "        logger.info(\"Using nn.parallel.DistributedDataParallel ...\")\n",
    "        \n",
    "        if params.encoder_only:\n",
    "            model = nn.parallel.DistributedDataParallel(model, device_ids=[params.local_rank], output_device=params.local_rank, broadcast_buffers=True)\n",
    "        else:\n",
    "            encoder = nn.parallel.DistributedDataParallel(encoder, device_ids=[params.local_rank], output_device=params.local_rank, broadcast_buffers=True)\n",
    "            decoder = nn.parallel.DistributedDataParallel(decoder, device_ids=[params.local_rank], output_device=params.local_rank, broadcast_buffers=True)\n",
    "\n",
    "    # build trainer, reload potential checkpoints / build evaluator\n",
    "    trainer = SingleTrainer(model, data, params)\n",
    "    evaluator = SingleEvaluator(trainer, data, params)\n",
    "        \n",
    "    # evaluation\n",
    "    if params.eval_only:\n",
    "        scores = evaluator.run_all_evals(trainer)\n",
    "        for k, v in scores.items():\n",
    "            logger.info(\"%s -> %.6f\" % (k, v))\n",
    "        logger.info(\"__log__:%s\" % json.dumps(scores))\n",
    "        exit()\n",
    "\n",
    "    # language model training\n",
    "    for _ in range(params.max_epoch):\n",
    "\n",
    "        logger.info(\"============ Starting epoch %i ... ============\" % trainer.epoch)\n",
    "\n",
    "        trainer.n_sentences = 0\n",
    "        trainer.n_images = 0\n",
    "        trainer.n_pairs = 0\n",
    "\n",
    "        while trainer.n_pairs < trainer.epoch_size:\n",
    "\n",
    "            # MLM step for language\n",
    "            for lang in params.mlm_steps:\n",
    "                trainer.mlm_step(lang, params.lambda_mlm)\n",
    "                \n",
    "            # MLM step for image \n",
    "            for img in params.ipm_steps:\n",
    "                trainer.ipm_step(img, params.lambda_ipm)\n",
    "                \n",
    "            # CMLM pretraining step\n",
    "            trainer.cmlm_step('cap', 'img', params.lambda_cmlm)\n",
    "          \n",
    "            trainer.iter()\n",
    "\n",
    "        logger.info(\"============ End of epoch %i ============\" % trainer.epoch)\n",
    "        \n",
    "        return data, trainer\n",
    "    \n",
    "        # evaluate perplexity and cross_entropy (combined accuracy)\n",
    "        scores = evaluator.run_all_evals(trainer)\n",
    "\n",
    "        # print / JSON log\n",
    "        for k, v in scores.items():\n",
    "            logger.info(\"%s -> %.6f\" % (k, v))\n",
    "        if params.is_master:\n",
    "            logger.info(\"__log__:%s\" % json.dumps(scores))\n",
    "\n",
    "        # end of epoch\n",
    "        trainer.save_best_model(scores)\n",
    "        trainer.save_periodic()\n",
    "        trainer.end_epoch(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/21/19 01:33:41 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 08/21/19 01:33:41 - 0:00:00 - ae_steps: \n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt_img: 256\n",
      "                                     bptt_word: 256\n",
      "                                     bt_steps: \n",
      "                                     clip_grad_norm: 5\n",
      "                                     cmlm_steps: [('cap', 'img')]\n",
      "                                     cmodal: cap-img\n",
      "                                     command: python /home/woenyon.lai/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py '-f' '/home/woenyon.lai/.local/share/jupyter/runtime/kernel-a2e83343-8c80-40b1-b5d1-dde4bdf5d573.json' --exp_id \"wt80at17cs\"\n",
      "                                     context_size: 0\n",
      "                                     crossmodal: ['cap', 'img']\n",
      "                                     crossmodal_dataset: {('cap', 'img'): {'cap': {'train': './data/processed/coco/cap/cap.train.pth', 'valid': './data/processed/coco/cap/cap.valid.pth', 'test': './data/processed/coco/cap/cap.test.pth'}, 'img': {'train': './data/processed/coco/img/train_features.hdf5', 'valid': './data/processed/coco/img/valid_features.hdf5', 'test': './data/processed/coco/img/test_features.hdf5'}}}\n",
      "                                     data_path: ./data/processed/coco/\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: ./dumped/cmlm/wt80at17cs\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 200000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: False\n",
      "                                     exp_id: wt80at17cs\n",
      "                                     exp_name: cmlm\n",
      "                                     features: 2048\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2modal: {0: 'cap', 1: 'img'}\n",
      "                                     images: ['vg']\n",
      "                                     img_dataset: {'vg': {'train': './data/processed/coco/vg/train.hdf5', 'valid': './data/processed/coco/vg/valid.hdf5', 'test': './data/processed/coco/vg/test.hdf5'}}\n",
      "                                     img_keep: 0.1\n",
      "                                     img_mask: 0.8\n",
      "                                     img_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     img_pred: 0.15\n",
      "                                     img_rand: 0.1\n",
      "                                     img_sample_alpha: 0\n",
      "                                     imgs: vg\n",
      "                                     ipm_steps: ['vg']\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_cmlm: 1\n",
      "                                     lambda_ipm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang_dataset: {'wiki': {'train': './data/processed/coco/wiki/en.train.pth', 'valid': './data/processed/coco/wiki/en.valid.pth', 'test': './data/processed/coco/wiki/en.test.pth'}}\n",
      "                                     langs: ['wiki']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lngs: wiki\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100000\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: ['wiki']\n",
      "                                     modal2id: {'cap': 0, 'img': 1}\n",
      "                                     mt_steps: \n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_bbox: 36\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_layers: 6\n",
      "                                     n_modality: 2\n",
      "                                     n_nodes: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     pc_steps: \n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     spatial_feat: 6\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl;_ce,10\n",
      "                                     tokens_per_batch: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl,_ce\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_sample_alpha: 0\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 08/21/19 01:33:41 - 0:00:00 - The experiment will be stored in ./dumped/cmlm/wt80at17cs\n",
      "                                     \n",
      "INFO - 08/21/19 01:33:41 - 0:00:00 - Running command: python /home/woenyon.lai/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py '-f' '/home/woenyon.lai/.local/share/jupyter/runtime/kernel-a2e83343-8c80-40b1-b5d1-dde4bdf5d573.json'\n",
      "\n",
      "WARNING - 08/21/19 01:33:41 - 0:00:00 - Signal handler installed.\n",
      "INFO - 08/21/19 01:33:41 - 0:00:00 - ============ Language data (wiki)\n",
      "INFO - 08/21/19 01:33:41 - 0:00:00 - Loading data from ./data/processed/coco/wiki/en.train.pth ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : gpudev001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/21/19 01:34:54 - 0:01:14 - 2768679201 words (52654 unique) in 43460511 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 08/21/19 01:36:15 - 0:02:35 - Loading data from ./data/processed/coco/wiki/en.valid.pth ...\n",
      "INFO - 08/21/19 01:36:15 - 0:02:35 - 322114 words (52654 unique) in 5000 sentences. 1 unknown words (1 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 08/21/19 01:36:16 - 0:02:35 - Loading data from ./data/processed/coco/wiki/en.test.pth ...\n",
      "INFO - 08/21/19 01:36:16 - 0:02:35 - 310141 words (52654 unique) in 5000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "\n",
      "\n",
      "INFO - 08/21/19 01:36:16 - 0:02:35 - ============ Image data (vg)\n",
      "\n",
      "INFO - 08/21/19 01:36:16 - 0:02:35 - ============ Cross modal data (cap-img)\n",
      "INFO - 08/21/19 01:36:16 - 0:02:35 - Loading data from ./data/processed/coco/cap/cap.train.pth ...\n",
      "INFO - 08/21/19 01:36:16 - 0:02:35 - 6230364 words (52654 unique) in 566435 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Loading data from ./data/processed/coco/cap/cap.valid.pth ...\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - 275359 words (52654 unique) in 25000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Loading data from ./data/processed/coco/cap/cap.test.pth ...\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - 274451 words (52654 unique) in 25000 sentences. 0 unknown words (0 unique) covering 0.00% of the data.\n",
      "\n",
      "\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - ============ Data summary\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Language data      - train -         wiki:  43460511\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Language data      - valid -         wiki:      5000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Language data      -  test -         wiki:      5000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - ==========================\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Image data         - train -           vg:   3062971 regions\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Image data         - valid -           vg:   1498648 regions\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Image data         -  test -           vg:   1508675 regions\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - ==========================\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   - train -   total_pair:    113287\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   - train -          cap:    566435\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   - train -          img:    113287\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - ============\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   - valid -   total_pair:      5000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   - valid -          cap:     25000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   - valid -          img:      5000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - ============\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   -  test -   total_pair:      5000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   -  test -          cap:     25000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - Cross-modal data   -  test -          img:      5000\n",
      "INFO - 08/21/19 01:36:16 - 0:02:36 - ============\n",
      "\n",
      "INFO - 08/21/19 01:36:22 - 0:02:41 - Number of parameters (model): 393945926\n",
      "INFO - 08/21/19 01:36:24 - 0:02:44 - ============ Starting epoch 0 ... ============\n",
      "INFO - 08/21/19 01:36:24 - 0:02:44 - Creating new training data iterator (pred,wiki) ...\n",
      "INFO - 08/21/19 01:37:02 - 0:03:22 - Creating new training data iterator (pred,vg) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\t\t\t: torch.Size([256, 32, 2048])\n",
      "y\t\t\t: torch.Size([1208])\n",
      "spatial_x\t\t: torch.Size([256, 32, 6])\n",
      "pred_mask\t\t: torch.Size([256, 32])\n",
      "candidates_before\t: torch.Size([1208, 2048])\n",
      "tensor_i\t\t: torch.Size([256, 32, 1024])\n",
      "candidates_after\t: torch.Size([1208, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.75 GiB (GPU 0; 15.90 GiB total capacity; 13.27 GiB already allocated; 329.88 MiB free; 1.73 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e8e3ce7bd0f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# run experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-0f2b8bd53f5f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# MLM step for image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipm_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipm_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_ipm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# CMLM pretraining step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jasper/cmlm/src/trainer.py\u001b[0m in \u001b[0;36mipm_step\u001b[0;34m(self, img, lambda_coeff)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;31m# number of processed sentences / words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jasper/cmlm/src/trainer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, loss, modules)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.75 GiB (GPU 0; 15.90 GiB total capacity; 13.27 GiB already allocated; 329.88 MiB free; 1.73 GiB cached)"
     ]
    }
   ],
   "source": [
    "# generate parser / parse parameters\n",
    "parser = get_parser()\n",
    "params = parser.parse_args(args=[])\n",
    "\n",
    "# jupyter\n",
    "params = modified_params(params)\n",
    "\n",
    "# check parameters\n",
    "check_data_params(params)\n",
    "check_model_params(params)\n",
    "\n",
    "# run experiment\n",
    "data, trainer = main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

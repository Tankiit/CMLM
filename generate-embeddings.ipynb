{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2019-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "#\n",
    "# Code to generate sentence representations from a pretrained model.\n",
    "# This can be used to initialize a cross-lingual classifier, for instance.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from src.utils import AttrDict\n",
    "from src.data.dictionary import Dictionary, BOS_WORD, EOS_WORD, PAD_WORD, UNK_WORD, MASK_WORD\n",
    "from src.model.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported languages: ar, bg, de, el, en, es, fr, hi, ru, sw, th, tr, ur, vi, zh\n"
     ]
    }
   ],
   "source": [
    "model_path = 'mlm_tlm_xnli15_1024.pth'\n",
    "reloaded = torch.load(model_path)\n",
    "params = AttrDict(reloaded['params'])\n",
    "print(\"Supported languages: %s\" % \", \".join(params.lang2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary / update parameters / build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary / update parameters\n",
    "dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "params.n_words = len(dico)\n",
    "params.bos_index = dico.index(BOS_WORD)\n",
    "params.eos_index = dico.index(EOS_WORD)\n",
    "params.pad_index = dico.index(PAD_WORD)\n",
    "params.unk_index = dico.index(UNK_WORD)\n",
    "params.mask_index = dico.index(MASK_WORD)\n",
    "\n",
    "# build model / reload weights\n",
    "model = TransformerModel(params, dico, True, True)\n",
    "model.eval()\n",
    "model.load_state_dict(reloaded['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get sentence representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences have to be in the BPE format, i.e. tokenized sentences on which you applied fastBPE.\n",
    "\n",
    "Below you can see an example for English, French, Spanish, German, Arabic and Chinese sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of (sentences, lang)\n",
    "sentences = [\n",
    "    ('the following secon@@ dary charac@@ ters also appear in the nov@@ el .', 'en'),\n",
    "    ('les zones rurales offr@@ ent de petites routes , a deux voies .', 'fr'),\n",
    "    ('luego del cri@@ quet , esta el futbol , el sur@@ f , entre otros .', 'es'),\n",
    "    ('am 18. august 1997 wurde der astero@@ id ( 76@@ 55 ) adam@@ ries nach ihm benannt .', 'de'),\n",
    "    ('اصدرت عدة افلام وث@@ اي@@ قية عن حياة السيدة في@@ روز من بينها :', 'ar'),\n",
    "    ('此外 ， 松@@ 嫩 平原 上 还有 许多 小 湖泊 ， 当地 俗@@ 称 为 “ 泡@@ 子 ” 。', 'zh'),\n",
    "]\n",
    "\n",
    "# add </s> sentence delimiters\n",
    "sentences = [(('</s> %s </s>' % sent.strip()).split(), lang) for sent, lang in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['</s>',\n",
       "   'the',\n",
       "   'following',\n",
       "   'secon@@',\n",
       "   'dary',\n",
       "   'charac@@',\n",
       "   'ters',\n",
       "   'also',\n",
       "   'appear',\n",
       "   'in',\n",
       "   'the',\n",
       "   'nov@@',\n",
       "   'el',\n",
       "   '.',\n",
       "   '</s>'],\n",
       "  'en'),\n",
       " (['</s>',\n",
       "   'les',\n",
       "   'zones',\n",
       "   'rurales',\n",
       "   'offr@@',\n",
       "   'ent',\n",
       "   'de',\n",
       "   'petites',\n",
       "   'routes',\n",
       "   ',',\n",
       "   'a',\n",
       "   'deux',\n",
       "   'voies',\n",
       "   '.',\n",
       "   '</s>'],\n",
       "  'fr'),\n",
       " (['</s>',\n",
       "   'luego',\n",
       "   'del',\n",
       "   'cri@@',\n",
       "   'quet',\n",
       "   ',',\n",
       "   'esta',\n",
       "   'el',\n",
       "   'futbol',\n",
       "   ',',\n",
       "   'el',\n",
       "   'sur@@',\n",
       "   'f',\n",
       "   ',',\n",
       "   'entre',\n",
       "   'otros',\n",
       "   '.',\n",
       "   '</s>'],\n",
       "  'es'),\n",
       " (['</s>',\n",
       "   'am',\n",
       "   '18.',\n",
       "   'august',\n",
       "   '1997',\n",
       "   'wurde',\n",
       "   'der',\n",
       "   'astero@@',\n",
       "   'id',\n",
       "   '(',\n",
       "   '76@@',\n",
       "   '55',\n",
       "   ')',\n",
       "   'adam@@',\n",
       "   'ries',\n",
       "   'nach',\n",
       "   'ihm',\n",
       "   'benannt',\n",
       "   '.',\n",
       "   '</s>'],\n",
       "  'de'),\n",
       " (['</s>',\n",
       "   'اصدرت',\n",
       "   'عدة',\n",
       "   'افلام',\n",
       "   'وث@@',\n",
       "   'اي@@',\n",
       "   'قية',\n",
       "   'عن',\n",
       "   'حياة',\n",
       "   'السيدة',\n",
       "   'في@@',\n",
       "   'روز',\n",
       "   'من',\n",
       "   'بينها',\n",
       "   ':',\n",
       "   '</s>'],\n",
       "  'ar'),\n",
       " (['</s>',\n",
       "   '此外',\n",
       "   '，',\n",
       "   '松@@',\n",
       "   '嫩',\n",
       "   '平原',\n",
       "   '上',\n",
       "   '还有',\n",
       "   '许多',\n",
       "   '小',\n",
       "   '湖泊',\n",
       "   '，',\n",
       "   '当地',\n",
       "   '俗@@',\n",
       "   '称',\n",
       "   '为',\n",
       "   '“',\n",
       "   '泡@@',\n",
       "   '子',\n",
       "   '”',\n",
       "   '。',\n",
       "   '</s>'],\n",
       "  'zh')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = len(sentences)\n",
    "slen = max([len(sent) for sent, _ in sentences])\n",
    "\n",
    "word_ids = torch.LongTensor(slen, bs).fill_(params.pad_index)\n",
    "for i in range(len(sentences)):\n",
    "    sent = torch.LongTensor([dico.index(w) for w in sentences[i][0]])\n",
    "    word_ids[:len(sent), i] = sent\n",
    "\n",
    "lengths = torch.LongTensor([len(sent) for sent, _ in sentences])\n",
    "langs = torch.LongTensor([params.lang2id[lang] for _, lang in sentences]).unsqueeze(0).expand(slen, bs) if params.n_langs > 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14],\n",
       "        [ 4,  6,  5,  2,  0, 14]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 6, 1024])\n"
     ]
    }
   ],
   "source": [
    "tensor = model('fwd', x=word_ids, lengths=lengths, langs=langs, causal=False).contiguous()\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `tensor` is of shape `(sequence_length, batch_size, model_dimension)`.\n",
    "\n",
    "`tensor[0]` is a tensor of shape `(batch_size, model_dimension)` that corresponds to the first hidden state of the last layer of each sentence.\n",
    "\n",
    "This is this vector that we use to finetune on the GLUE and XNLI tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
